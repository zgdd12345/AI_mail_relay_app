"""Utilities for parsing arXiv alert emails."""

from __future__ import annotations

import re
from dataclasses import dataclass, field
from typing import Iterable, List, Sequence


PAPER_SPLIT_RE = re.compile(r"^Title:\s", re.IGNORECASE)
CATEGORY_RE = re.compile(r"([a-z\-]+\.[A-Z0-9\-]+)")
LINK_RE = re.compile(r"https?://\S+")
ARXIV_ID_RE = re.compile(r"arXiv:(\d+\.\d+)")


@dataclass
class ArxivPaper:
    """Structured representation of a single arXiv paper entry."""

    title: str
    authors: str
    categories: List[str]
    abstract: str
    links: List[str] = field(default_factory=list)
    affiliations: str = ""  # Author affiliations/institutions
    arxiv_id: str = ""  # arXiv ID (e.g., 2510.24832)
    summary: str = ""  # One-sentence summary of the work (generated by LLM)

    def matches_category(self, allowed_categories: Sequence[str]) -> bool:
        allowed = {c.lower() for c in allowed_categories}
        return any(category.lower() in allowed for category in self.categories)

    def matches_keyword(self, keywords: Sequence[str]) -> bool:
        haystack = " ".join([self.title, self.abstract]).lower()
        return any(keyword.lower() in haystack for keyword in keywords)


def _split_entries(text: str) -> List[str]:
    segments: List[str] = []
    current: List[str] = []

    for line in text.splitlines():
        if PAPER_SPLIT_RE.match(line):
            if current:
                segments.append("\n".join(current).strip())
                current = []
        current.append(line)

    if current:
        segments.append("\n".join(current).strip())

    return [segment for segment in segments if segment]


def parse_arxiv_email(body: str) -> List[ArxivPaper]:
    """Parse the plain-text body of an arXiv subscription email."""
    entries = _split_entries(body)
    papers: List[ArxivPaper] = []

    for entry in entries:
        lines = entry.splitlines()
        metadata: dict[str, str] = {}
        abstract_lines: List[str] = []
        collecting_abstract = False

        # Extract arXiv ID from entry
        arxiv_id_match = ARXIV_ID_RE.search(entry)
        arxiv_id = arxiv_id_match.group(1) if arxiv_id_match else ""

        for line in lines:
            if line.lower().startswith("title:"):
                metadata["title"] = line.split(":", 1)[1].strip()
                collecting_abstract = False
            elif line.lower().startswith("authors:"):
                metadata["authors"] = line.split(":", 1)[1].strip()
                collecting_abstract = False
            elif line.lower().startswith("categories:"):
                metadata["categories"] = line.split(":", 1)[1].strip()
                collecting_abstract = False
            elif line.lower().startswith("comments:"):
                # Extract affiliation info from comments if available
                metadata["comments"] = line.split(":", 1)[1].strip()
                collecting_abstract = False
            elif line.lower().startswith("abstract:"):
                collecting_abstract = True
                abstract_lines.append(line.split(":", 1)[1].strip())
            elif line.strip() == "\\\\":
                # arXiv daily digest uses '\\' to mark start of abstract
                collecting_abstract = True
            elif collecting_abstract:
                if line.strip() == "":
                    collecting_abstract = False
                else:
                    abstract_lines.append(line.strip())

        if not metadata.get("title"):
            continue

        # Store abstract even if empty (some papers might not have abstracts)
        metadata["abstract"] = " ".join(abstract_lines).strip()

        raw_categories = metadata.get("categories", "")
        categories = CATEGORY_RE.findall(raw_categories) or [
            token.strip() for token in raw_categories.split() if token
        ]

        paper = ArxivPaper(
            title=metadata.get("title", ""),
            authors=metadata.get("authors", ""),
            categories=categories,
            abstract=" ".join(abstract_lines).strip(),
            links=LINK_RE.findall(entry),
            affiliations=metadata.get("comments", ""),  # Use comments field for affiliations
            arxiv_id=arxiv_id,
        )
        papers.append(paper)

    return papers


def filter_papers(
    papers: Iterable[ArxivPaper],
    allowed_categories: Sequence[str],
    keyword_filters: Sequence[str],
) -> List[ArxivPaper]:
    """Return AI-relevant papers based on configured filters."""
    filtered: List[ArxivPaper] = []
    for paper in papers:
        if allowed_categories and paper.matches_category(allowed_categories):
            filtered.append(paper)
        elif keyword_filters and paper.matches_keyword(keyword_filters):
            filtered.append(paper)
    return filtered


__all__ = ["ArxivPaper", "parse_arxiv_email", "filter_papers"]
